{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to handle Nan-values so that the HA doesn't get marginalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been a problem this far, that the clustering doesn't work as desired, and the problem is now located in the procedure Nan-values have been handled in clustering. So we need a better way to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from gradutil import *\n",
    "from pyomo.opt import SolverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedn = 2\n",
    "opt = SolverFactory('glpk')\n",
    "solutions = real_solutions()\n",
    "revenue, carbon, deadwood, ha = init_boreal()\n",
    "x = pd.concat((revenue, carbon, deadwood, ha), axis=1)\n",
    "x_bau = pd.concat((nan_to_bau(revenue), nan_to_bau(carbon), nan_to_bau(deadwood), nan_to_bau(ha)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Nan:s separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan = x.iloc[np.any(np.isnan(x), axis=1),:]\n",
    "x_nan_bau = x_bau.iloc[np.any(np.isnan(x), axis=1),:]\n",
    "type(x_nan), np.shape(x_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num = x.iloc[np.all(~np.isnan(x), axis=1),:]\n",
    "np.shape(x_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cluster all the different \"nan-scenarios\" differently. Details on how the nan:s relate are found in the DataTesting notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nany2y4y6 = x_nan.values[np.logical_and(np.logical_and(np.isnan(x_nan.iloc[:,2]),np.isnan(x_nan.iloc[:,4])), np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_nany2y4n6 = x_nan.values[np.logical_and(np.logical_and(np.isnan(x_nan.iloc[:,2]),np.isnan(x_nan.iloc[:,4])), ~np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_nany2n4y6 = x_nan.values[np.logical_and(np.logical_and(np.isnan(x_nan.iloc[:,2]),~np.isnan(x_nan.iloc[:,4])), np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_nany2n4n6 = x_nan.values[np.logical_and(np.logical_and(np.isnan(x_nan.iloc[:,2]),~np.isnan(x_nan.iloc[:,4])), ~np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_nann2y4y6 = x_nan.values[np.logical_and(np.logical_and(~np.isnan(x_nan.iloc[:,2]),np.isnan(x_nan.iloc[:,4])), np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_nann2y4n6 = x_nan.values[np.logical_and(np.logical_and(~np.isnan(x_nan.iloc[:,2]),np.isnan(x_nan.iloc[:,4])), ~np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_nann2n4y6 = x_nan.values[np.logical_and(np.logical_and(~np.isnan(x_nan.iloc[:,2]),~np.isnan(x_nan.iloc[:,4])), np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_nann2n4n6 = x_nan.values[np.logical_and(np.logical_and(~np.isnan(x_nan.iloc[:,2]),~np.isnan(x_nan.iloc[:,4])), ~np.isnan(x_nan.iloc[:,6])),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_nany2y4y6), np.shape(x_nany2y4n6), np.shape(x_nany2n4y6), np.shape(x_nany2n4n6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_nann2y4y6), np.shape(x_nann2y4n6), np.shape(x_nann2n4y6), np.shape(x_nann2n4n6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combinations we need are then: x_nany2y4y6, x_nany2y4n6, x_nann2y4n6, x_nann2n4y6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_nany2y4y6)[0]+np.shape(x_nany2y4n6)[0]+np.shape(x_nann2y4n6)[0]+np.shape(x_nann2n4y6)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually last three of these nan-versions are so small, that there is probably no point to cluster them anymore. So we can assume them as single clusters for the optimization part. The biggest one should still be splitted a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan1 = x_nany2y4y6\n",
    "x_nan2 = x_nany2y4n6\n",
    "x_nan3 = x_nann2y4n6\n",
    "x_nan4 = x_nann2n4y6\n",
    "\n",
    "x_bau1 = x_nan_bau.values[np.logical_and(np.logical_and( np.isnan(x_nan.iloc[:,2]), np.isnan(x_nan.iloc[:,4])), np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_bau2 = x_nan_bau.values[np.logical_and(np.logical_and( np.isnan(x_nan.iloc[:,2]), np.isnan(x_nan.iloc[:,4])),~np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_bau3 = x_nan_bau.values[np.logical_and(np.logical_and(~np.isnan(x_nan.iloc[:,2]), np.isnan(x_nan.iloc[:,4])),~np.isnan(x_nan.iloc[:,6])),:]\n",
    "x_bau4 = x_nan_bau.values[np.logical_and(np.logical_and(~np.isnan(x_nan.iloc[:,2]),~np.isnan(x_nan.iloc[:,4])), np.isnan(x_nan.iloc[:,6])),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all the columns containing (only) Nans, and normalize column wise to the 0-1 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = np.ones(len(x_nan1[0]), dtype=bool)\n",
    "mask[np.isnan(x_nan1[0])] = False\n",
    "clust_x_nan1 = x_nan1[:,mask]\n",
    "norm_clust_nan1 = normalize(clust_x_nan1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "nclust1 = 100\n",
    "c, xtoc, dist = cluster(norm_clust_nan1, nclust1, seedn, verbose=1)\n",
    "\n",
    "w_nan1 = np.array([sum(xtoc == i) for i in range(len(c))])\n",
    "\n",
    "c_nan1 = np.array([x_bau1[xtoc == i].mean(axis=0) for i in range(nclust)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now because we are using reduced data for the clustering, we need to change the centers to correspond to the dataset where Nans are replaced with Bau values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nan2 = x_bau2.mean(axis=0)\n",
    "c_nan3 = x_bau3.mean(axis=0)\n",
    "c_nan4 = x_bau4.mean(axis=0)\n",
    "\n",
    "w_nan2 = np.shape(x_nan2)[0]\n",
    "w_nan3 = np.shape(x_nan3)[0]\n",
    "w_nan4 = np.shape(x_nan4)[0]\n",
    "\n",
    "combined_data = np.concatenate((c_nan1,np.array((c_nan2, c_nan3, c_nan4))), axis=0)\n",
    "combined_weights = np.concatenate((w_nan1, np.array((w_nan2, w_nan3, w_nan4))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_x = np.concatenate((x_nan1, x_nan2, x_nan3, x_nan4), axis=0)\n",
    "res_xtoc = np.concatenate((xtoc, \n",
    "                           np.ones(np.shape(x_nan2)[0])*(nclust), \n",
    "                           np.ones(np.shape(x_nan3)[0])*(nclust+1), \n",
    "                           np.ones(np.shape(x_nan4)[0])*(nclust+2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = SolverFactory('glpk')\n",
    "\n",
    "prob1, prob2, prob3, prob4 = optimize_all(normalize(combined_data), combined_weights, opt)\n",
    "\n",
    "val1 = model_to_real_values(x_nan_bau.iloc[:, :7].values, res_xtoc, prob1[0].model)\n",
    "val2 = model_to_real_values(x_nan_bau.iloc[:, 7:14].values, res_xtoc, prob2[0].model)\n",
    "val3 = model_to_real_values(x_nan_bau.iloc[:, 14:21].values, res_xtoc, prob3[0].model)\n",
    "val4 = model_to_real_values(x_nan_bau.iloc[:, 21:].values, res_xtoc, prob4[0].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_num_x = normalize(x_num.values)\n",
    "norm_nan_x = normalize(x_nan.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the optimization result straight away using only lines with nan-values, so that we can compare clustering results to something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "opt = SolverFactory('glpk')\n",
    "real_nan_revenue, real_nan_carbon, real_nan_deadwood, real_nan_ha = optimize_all(norm_nan_x, np.ones(len(norm_nan_x)), opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_list = values_to_list(real_nan_revenue[0], x_nan_bau.iloc[:,:7].values)\n",
    "carbon_list = values_to_list(real_nan_carbon[0], x_nan_bau.iloc[:,7:14].values)\n",
    "deadwood_list = values_to_list(real_nan_deadwood[0], x_nan_bau.iloc[:,14:21].values)\n",
    "ha_list = values_to_list(real_nan_ha[0], x_nan_bau.iloc[:,21:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Relative differences to original values (calculated with Nan:s), 100 clusters')\n",
    "print(\"(i)   Harvest revenues {:.3f}\".format((val1-sum(revenue_list))/sum(revenue_list)))\n",
    "print(\"(ii)  Carbon storage   {:.3f}\".format((val2-sum(carbon_list))/sum(carbon_list)))\n",
    "print(\"(iii) Deadwood index   {:.3f}\".format((val3-sum(deadwood_list))/sum(deadwood_list)))\n",
    "print(\"(iv)  Combined Habitat {:.3f}\".format((val4-sum(ha_list))/sum(ha_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative differences to original values (calculated with Nan:s), 50 clusters\n",
    "- (i)   Harvest revenues -0.003\n",
    "- (ii)  Carbon storage   -0.086\n",
    "- (iii) Deadwood index   -0.405\n",
    "- (iv)  Combined Habitat 0.036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative differences to original values (calculated with Nan:s), 25 clusters\n",
    "- (i)   Harvest revenues 0.016\n",
    "- (ii)  Carbon storage   -0.085\n",
    "- (iii) Deadwood index   -0.437\n",
    "- (iv)  Combined Habitat 0.081\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the differences between objectives are not so great. Also now the biggest problem is in the Deadwood and no longer HA. Well, that is some kind of progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then try also clustering Nan-lines with just replacing Nan:s with BAUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nclust2 = 500\n",
    "n_nan_opt_revenue, n_nan_opt_carbon, n_nan_opt_deadwood, n_nan_opt_ha = cNopt(x_nan_bau.values, x_nan_bau.values, x_nan_bau.values, opt, nclust2, seedn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Relative differences to original values (calculated with Nan:s), 50 clusters')\n",
    "print(\"(i) Harvest revenues difference {:.3f}\".format((n_nan_opt_revenue-sum(revenue_list))/sum(revenue_list)))\n",
    "print(\"(ii) Carbon storage {:.3f}\".format((n_nan_opt_carbon-sum(carbon_list))/sum(carbon_list)))\n",
    "print(\"(iii) Deadwood index {:.3f}\".format((n_nan_opt_deadwood-sum(deadwood_list))/sum(deadwood_list)))\n",
    "print(\"(iv) Combined Habitat {:.3f}\".format((n_nan_opt_ha-sum(ha_list))/sum(ha_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are somehow similar to the previous ones, meaning that relative differences are not so great and the worst one ise Deadwood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and running all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now cluster and optimize everything just by replacing Nan values with BAU values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclust3 = 50\n",
    "all_revenue, all_carbon, all_deadwood, all_ha = cNopt(x_bau.values, x_bau.values, x_bau.values, opt, nclust3, seedn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Relative differences to original values clustering all, 50 clusters')\n",
    "print(\"(i) Harvest revenues difference {:.3f}\".format((all_revenue-solutions['revenue'])/solutions['revenue']))\n",
    "print(\"(ii) Carbon storage {:.3f}\".format((all_carbon-solutions['carbon'])/solutions['carbon']))\n",
    "print(\"(iii) Deadwood index {:.3f}\".format((all_deadwood-solutions['deadwood'])/solutions['deadwood']))\n",
    "print(\"(iv) Combined Habitat {:.3f}\".format((all_ha-solutions['ha'])/solutions['ha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster separately and optimize together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try clustering all the lines including Nan:s using only columns that are not Nan:s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = np.ones(len(x_nan1[0]), dtype=bool)\n",
    "mask[np.isnan(x_nan1[0])] = False\n",
    "mask[np.isnan(x_nan2[0])] = False\n",
    "mask[np.isnan(x_nan3[0])] = False\n",
    "mask[np.isnan(x_nan4[0])] = False\n",
    "clust_x_nan = x_nan.values[:,mask]\n",
    "norm_clust_nan = normalize(clust_x_nan)\n",
    "norm_clust_num = normalize(x_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "nclust4 = 500\n",
    "c_nan, xtoc_nan, dist_nan = cluster(norm_clust_nan, nclust4, seedn, verbose=1)\n",
    "c_num, xtoc_num, dist_num = cluster(norm_clust_num, nclust4, seedn, verbose=1)\n",
    "\n",
    "w_nan = np.array([sum(xtoc_nan == i) for i in range(len(c_nan))])\n",
    "c_nan = np.array([x_nan_bau.iloc[xtoc_nan == i].mean(axis=0) for i in range(nclust4)])\n",
    "\n",
    "w_num = np.array([sum(xtoc_num == i) for i in range(len(c_num))])\n",
    "c_num = np.array([x_num.iloc[xtoc_num == i].mean(axis=0) for i in range(nclust4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_x = np.concatenate((c_nan, c_num), axis=0)\n",
    "combined_weights_all = np.concatenate((w_nan, w_num), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_xtoc_all = np.concatenate((xtoc_nan, xtoc_num+nclust4), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = SolverFactory('glpk')\n",
    "\n",
    "prob1_all, prob2_all, prob3_all, prob4_all = optimize_all(normalize(combined_x), combined_weights_all, opt)\n",
    "\n",
    "val1_all = model_to_real_values(x_bau.iloc[:, :7].values, res_xtoc_all, prob1_all[0].model)\n",
    "val2_all = model_to_real_values(x_bau.iloc[:, 7:14].values, res_xtoc_all, prob2_all[0].model)\n",
    "val3_all = model_to_real_values(x_bau.iloc[:, 14:21].values, res_xtoc_all, prob3_all[0].model)\n",
    "val4_all= model_to_real_values(x_bau.iloc[:, 21:].values, res_xtoc_all, prob4_all[0].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Relative differences to original values clustering all, 50 clusters')\n",
    "print(\"(i) Harvest revenues difference {:.3f}\".format((val1_all-solutions['revenue'])/solutions['revenue']))\n",
    "print(\"(ii) Carbon storage {:.3f}\".format((val2_all-solutions['carbon'])/solutions['carbon']))\n",
    "print(\"(iii) Deadwood index {:.3f}\".format((val3_all-solutions['deadwood'])/solutions['deadwood']))\n",
    "print(\"(iv) Combined Habitat {:.3f}\".format((val4_all-solutions['ha'])/solutions['ha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final words from this testing are as follows: Just replacing NAN values with BAU values in clustering and optimization works better than previous approaches. Even better options is to cluster all the lines containing some Nan-values separately using just columns without any Nan:s. This far the best option is to cluster all the 'Nan-classes' separately, using spesific number of features for every class."
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}