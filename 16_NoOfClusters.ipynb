{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the proper number of clusters, so that system to be used in the live session will be justified. We are going to do this by clustering all the objectives separately and selecting correct number of clusters for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ASF import ASF\n",
    "from gradutil import *\n",
    "from pyomo.opt import SolverFactory\n",
    "from BorealWeights import BorealWeightedProblem\n",
    "seedn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "revenue, carbon, deadwood, ha = init_boreal()\n",
    "n_revenue = nan_to_bau(revenue)\n",
    "n_carbon= nan_to_bau(carbon)\n",
    "n_deadwood = nan_to_bau(deadwood)\n",
    "n_ha = nan_to_bau(ha)\n",
    "ide = ideal(False)\n",
    "nad = nadir(False)\n",
    "opt = SolverFactory('glpk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat((n_revenue, n_carbon, n_deadwood, n_ha), axis=1)\n",
    "x_stack = np.dstack((n_revenue, n_carbon, n_deadwood, n_ha))\n",
    "\n",
    "x_norm = normalize(x.values)\n",
    "x_norm_stack = normalize(x_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of clusters that keeps the user waiting time less than a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import time\n",
    "dur = 0\n",
    "nclust1 = 50\n",
    "while dur < 10:\n",
    "    nclust1 += 10\n",
    "    c, xtoc, dist = cluster(x_norm, nclust1, seedn, verbose=0)\n",
    "    w = np.array([sum(xtoc == i) for i in range(nclust1)])\n",
    "    c_mean = np.array([x_norm_stack[xtoc == i].mean(axis=0) for i in range(nclust1)])\n",
    "    start = time.time()\n",
    "    ref = np.array((ide[0], 0, 0, 0))\n",
    "    asf = ASF(ide, nad, ref, c_mean, weights=w)\n",
    "    stom = ASF(ide, nad, ref, c_mean, weights=w, scalarization='stom')\n",
    "    guess = ASF(ide, nad, ref, c_mean, weights=w, scalarization='guess')\n",
    "    opt.solve(asf.model)\n",
    "    opt.solve(stom.model)\n",
    "    opt.solve(guess.model)\n",
    "    dur = time.time() - start\n",
    "print(nclust1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if possible, we try to keep the total number of clusters below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_and_eval(x, x_opt, x_orig, rng, seeds=range(2,11)):\n",
    "    distsum = []\n",
    "    optires = []\n",
    "    opt = SolverFactory('cplex')\n",
    "    for nclust in rng:\n",
    "        dists = []\n",
    "        optis = []\n",
    "        for seedn in seeds:\n",
    "            c, xtoc, dist = cluster(x, nclust, seedn, verbose=0)\n",
    "            w = np.array([sum(xtoc == i) for i in range(nclust) if sum(xtoc==i) > 0])\n",
    "            c_close = np.array([x_opt[np.argmin(dist[xtoc == i])] for i in range(nclust) if len(dist[xtoc == i]) > 0])\n",
    "            prob = BorealWeightedProblem(c_close, weights=w)\n",
    "            res = opt.solve(prob.model)\n",
    "            optis.append(model_to_real_values(x_orig, prob.model, xtoc))\n",
    "            dists.append(np.nansum(dist))\n",
    "        optires.append(optis)\n",
    "        distsum.append(dists)\n",
    "    return distsum, optires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = range(50,251,20)\n",
    "seeds = range(2,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distsum_revenue, optires_revenue = kmeans_and_eval(x_norm[:,:7],x_norm[:,:7],x.values[:,:7], rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distsum_carbon, optires_carbon = kmeans_and_eval(x_norm[:,7:14],x_norm[:,7:14],x.values[:,7:14], rng, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_deadwood, optires_deadwood = kmeans_and_eval(x_norm[:,14:21],x_norm[:,14:21],x.values[:,14:21], rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_ha, optires_ha = kmeans_and_eval(x_norm[:,21:],x_norm[:,21:],x.values[:,21:], rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15,12)\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "fig.suptitle('Number of clusters and sum of intra cluster distances')\n",
    "\n",
    "ax[0,0].plot(rng, np.mean(distsum_revenue, axis=1))\n",
    "ax[0,0].set_title('Revenue')\n",
    "\n",
    "ax[0,1].plot(rng, np.mean(distsum_carbon, axis=1))\n",
    "ax[0,1].set_title('Carbon')\n",
    "\n",
    "ax[1,0].plot(rng, np.mean(distsum_deadwood, axis=1))\n",
    "ax[1,0].set_title('Deadwood')\n",
    "\n",
    "ax[1,1].plot(rng, np.mean(distsum_ha, axis=1))\n",
    "ax[1,1].set_title('HA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2)\n",
    "fig.suptitle('Number of clusters and minimu, average and maximum of optimization results')\n",
    "\n",
    "data = np.array([[optires_revenue, optires_carbon], [optires_deadwood, optires_ha]])\n",
    "names = np.array([['Revenue', 'Carbon'],['Deadwood', 'Habitat']])\n",
    "for i in range(np.shape(ax)[0]):\n",
    "    for j in range(np.shape(ax)[1]):\n",
    "        ax[i,j].plot(rng, [k for k in zip(np.max(data[i,j], axis=1), np.mean(data[i,j], axis=1), np.min(data[i,j], axis=1))])\n",
    "        ax[i,j].set_title(names[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots we can say that more clusters we have the \"better\" the clusters are. From the optimization perspective, the results are actually not getting better with more clusters for all the objectives. For Revenue and Carbon they do, but for Deadwood and HA not really. This should be studied with even more clusters sometime.\n",
    "There has also before aroused some issues with Deadwood and HA values, and this is the issue now again.\n",
    "\n",
    "The dispersion of results is decreasing with the increase of clusters. It is of course good news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays we also have the map data, so we could use it in this also. We could get better results, but still it is more data handling and not so much contributing in to this thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can conclude this by saying that we just fix the number of clusters to be as big as we want, which is about 60 clusters in this case. (Keeping calculation time under 1 sec.)"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}