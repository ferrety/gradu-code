{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the proper number of clusters, so that system to be used in the live session will be justified. We are going to do this by clustering all the objectives separately and selecting correct number of clusters for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ASF import ASF\n",
    "from gradutil import *\n",
    "from pyomo.opt import SolverFactory\n",
    "seedn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "revenue, carbon, deadwood, ha = init_boreal()\n",
    "n_revenue = nan_to_bau(revenue)\n",
    "n_carbon= nan_to_bau(carbon)\n",
    "n_deadwood = nan_to_bau(deadwood)\n",
    "n_ha = nan_to_bau(ha)\n",
    "ide = ideal(False)\n",
    "nad = nadir(False)\n",
    "opt = SolverFactory('glpk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat((n_revenue, n_carbon, n_deadwood, n_ha), axis=1)\n",
    "x_stack = np.dstack((n_revenue, n_carbon, n_deadwood, n_ha))\n",
    "\n",
    "x_norm = normalize(x.values)\n",
    "x_norm_stack = normalize(x_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of clusters that keeps the user waiting time less than a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import time\n",
    "dur = 0\n",
    "nclust1 = 50\n",
    "while dur < 1:\n",
    "    nclust1 += 50\n",
    "    c, xtoc, dist = cluster(x_norm, nclust1, seedn, verbose=0)\n",
    "    w = np.array([sum(xtoc == i) for i in range(nclust1)])\n",
    "    c_mean = np.array([x_norm_stack[xtoc == i].mean(axis=0) for i in range(nclust1)])\n",
    "    start = time.time()\n",
    "    ref = np.array((ide[0], 0, 0, 0))\n",
    "    asf = ASF(ide, nad, ref, c_mean, weights=w)\n",
    "    opt.solve(asf.model)\n",
    "    dur = time.time() - start\n",
    "print(nclust1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if possible, we try to keep the total number of clusters below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_and_eval(x, rng):\n",
    "    distsum = []\n",
    "    for nclust in rng:\n",
    "        c, xtoc, dist = cluster(x, nclust, seedn, verbose=0)\n",
    "        distsum.append(np.nansum(dist))\n",
    "    return distsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = range(50,251,20)\n",
    "distsum_revenue = kmeans_and_eval(x_norm[:,:7], rng)\n",
    "distsum_carbon = kmeans_and_eval(x_norm[:,7:14], rng)\n",
    "distsum_deadwood = kmeans_and_eval(x_norm[:,14:21], rng)\n",
    "distsum_ha = kmeans_and_eval(x_norm[:,21:], rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15,12)\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "fig.suptitle('Number of clusters and sum of intra cluster distances')\n",
    "\n",
    "ax[0,0].plot(rng, distsum_revenue)\n",
    "ax[0,0].set_title('Revenue')\n",
    "\n",
    "ax[0,1].plot(rng, distsum_carbon)\n",
    "ax[0,1].set_title('Carbon')\n",
    "\n",
    "ax[1,0].plot(rng, distsum_deadwood)\n",
    "ax[1,0].set_title('Deadwood')\n",
    "\n",
    "ax[1,1].plot(rng, distsum_ha)\n",
    "ax[1,1].set_title('HA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots we can say nothing but that more is more. The more we have clusters, the more accurate the results are. We also see, that k-means doesn't handle HA values so nicely, at least when compared to the revenue values. All the variables are normalized to 0-1 scale, so that cannot be the reason. There is just something nasty in the data (HA indices are calculated using some nonlinear approximations, so they are not handled as gracefully in this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays we also have the map data, so we could use it in this also. We could get better results, but still it is more data handling and not so much contributing in to this thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can conclude this by saying that we just fix the number of clusters to be as big as we want, which is 200 clusters in this case. (Keeping calculation time under 1 sec.)"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}