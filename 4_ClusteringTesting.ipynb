{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial idea is to cluster all the stands according to their similarity and then to form own surrogate for every cluster. That of course rises somes questions:\n",
    "- what is the similarity measure?\n",
    "- what is good cluster size?\n",
    "- how do use surrogates in the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because clusters and then also surrogates must be created automatically we need some way to measure and compare different clusterings and surrogates. Best way to do that would be feeding the right away in to the optimization and then see how they compare between each other and the solution in the paper. Then we just would need the optimization procedure first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), '../boreal_data')\n",
    "\n",
    "carbon = pd.read_csv(os.path.join(data_dir, 'Carbon_storage.csv'))\n",
    "HA = pd.read_csv(os.path.join(data_dir, 'Combined_HA.csv'))\n",
    "deadwood = pd.read_csv(os.path.join(data_dir, 'Deadwood_volume.csv'))\n",
    "revenue = pd.read_csv(os.path.join(data_dir, 'Timber_revenues.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = carbon.copy()\n",
    "X1[carbon.isnull()] = np.nanmin(carbon.values) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cosine metric is used, because we want to ignore the size of stands and prefer their similarity in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z100 = linkage(X1[:100], metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c100, coph_dists = cophenet(Z100, pdist(X1[:100]))\n",
    "c100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cophenet distance is quite close to 1 so there is no need to be worried (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15,6)\n",
    "plt.figure()\n",
    "dendrogram(Z100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this works with small data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now question: What is this green cluster?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "clusters = fcluster(Z100, 0.14,criterion='distance')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon[:100][clusters==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon[:100][clusters==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon[:100][clusters==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks quite great already!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just one problem: now we assigned Nan-values to be smalles value - 1. This means Nan:s are not a big difference when compared to other values in the dataset -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(carbon.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmax(carbon.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, differences between 'valid' data points are much greater than between 'valid' points and points with Nan-values. So it would make sense to assign much different values for Nan:s. That would also automatically connect all the Nan-including lines to the same clusters. Of course another option is to run this clustering separately for all the lines with Nan:s and all the lines without Nan:s. I am just not sure if assigning greatly different values is more efficient or more general than doing this separately. This should be studied!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare some timings with different sized datasets and clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Z100 = linkage(X1[:100], metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Z1000 = linkage(X1[:1000], metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Z10000 = linkage(X1[:10000], metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Z20000 = linkage(X1[:20000], metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Zall = linkage(X1, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15,9)\n",
    "plt.figure()\n",
    "dendrogram(Zall, truncate_mode='lastp', p=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "clusters_all = fcluster(Zall, 50 ,criterion='maxclust')\n",
    "clusters_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 1\n",
    "print(len(carbon[clusters_all==ind]))\n",
    "carbon[clusters_all==ind][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "29666*0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was said that 35% of stands were simulated. Would all stands belonging to the first cluster be those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's also try hierarchical clustering by assigning much worse values for Nan.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = carbon.copy()\n",
    "X2[carbon.isnull()] = np.nanmin(carbon.values) - np.nanmax(carbon.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zall_diff = linkage(X2, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15,9)\n",
    "plt.figure()\n",
    "dendrogram(Zall_diff, truncate_mode='lastp', p=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "clusters_diff = fcluster(Zall_diff, 50 ,criterion='maxclust')\n",
    "clusters_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 49\n",
    "print(len(carbon[clusters_diff==ind]))\n",
    "carbon[clusters_diff==ind][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_stands(id1, id2):\n",
    "    return (carbon[clusters_diff==49].iloc[id2,0]/carbon[clusters_diff==49].iloc[id1,0],\n",
    "            carbon[clusters_diff==49].iloc[id2,1]/carbon[clusters_diff==49].iloc[id1,1],\n",
    "            carbon[clusters_diff==49].iloc[id2,5]/carbon[clusters_diff==49].iloc[id1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_variances = [var(compare_stands(0,i)) for i in range(1,len(carbon[clusters_diff==49]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum variance: {},\\nminimun variance: {},\\nmedian variance: {}'.format(max(inside_variances), min(inside_variances), np.median(inside_variances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_variances_all = [var(compare_stands(0,i)) for i in range(1,len(carbon[clusters_all==1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum variance: {},\\nminimun variance: {},\\nmedian variance: {}'.format(max(inside_variances), min(inside_variances), np.median(inside_variances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 5568\n",
    "((clusters_all==1)[:ind]==(clusters_diff==49)[:ind]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 6653\n",
    "((clusters_all==1)[ind:]==(clusters_diff==49)[ind:]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "((clusters_all==1)==(clusters_diff==49)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans,vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data100 = X1[:100]\n",
    "centroids100, _ = kmeans(data100, 50)\n",
    "idx100,  _ = vq(data100, centroids100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data1000 = X1[:1000]\n",
    "centroids1000, _ = kmeans(data1000, 50)\n",
    "idx1000,  _ = vq(data1000, centroids1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data10000 = X1[:10000]\n",
    "centroids10000, _ = kmeans(data10000, 50)\n",
    "idx10000,  _ = vq(data10000, centroids10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_all = X1\n",
    "centroidsall, _ = kmeans(data_all, 50)\n",
    "idxall,  _ = vq(data_all, centroidsall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem is, it is not possible to define cosine distance using scipy or sklearn. Luckily I found this one kmeans implementation from the web: https://stackoverflow.com/questions/5529625/is-it-possible-to-specify-your-own-distance-function-using-scikit-learn-k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmeans import kmeans, randomsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "randomcenters = randomsample(data_all.values, 50)\n",
    "centers, xtoc, dist = kmeans(data_all.values, randomcenters, delta=.001, maxiter=100, metric='cosine', verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is remarkably faster than hierarchical clustering. K-means's weakness is the need to know number of clusters before hand. Anyway if I don't really have to replicate the data and its real structure, I can just assign as big number of clusters as possible to still solve MILP program. If the number is big enough the error will be small enough to get relatively reliable results."
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}